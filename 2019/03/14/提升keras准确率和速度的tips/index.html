<!DOCTYPE html><html lang="zh-cn"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Hi!Roy!"><title>提升keras准确率和速度的小tips - Hi!Roy!</title><meta name="author" content="Roy"><link rel="icon" href="http://www.hi-roy.com/assets/images/favicon.ico"><link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml"><meta name="description" content="这里记录一下对于新手(对,说的就是本人)学习kears框架时用来提升准确率的一些tip,但这里都是”术”的层面,而对于”道”,还是要看数学.全文以深度学习界的”hello world”-手写数字识别为例."><meta name="keywords" content="python,kears"><meta property="og:type" content="blog"><meta property="og:title" content="提升keras准确率和速度的小tips"><meta property="og:url" content="http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/index.html"><meta property="og:site_name" content="Hi!Roy!"><meta property="og:description" content="这里记录一下对于新手(对,说的就是本人)学习kears框架时用来提升准确率的一些tip,但这里都是”术”的层面,而对于”道”,还是要看数学.全文以深度学习界的”hello world”-手写数字识别为例."><meta property="og:locale" content="zh-cn"><meta property="og:updated_time" content="2019-09-16T08:11:51.526Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="提升keras准确率和速度的小tips"><meta name="twitter:description" content="这里记录一下对于新手(对,说的就是本人)学习kears框架时用来提升准确率的一些tip,但这里都是”术”的层面,而对于”道”,还是要看数学.全文以深度学习界的”hello world”-手写数字识别为例."><meta property="og:image" content="http://www.hi-roy.com/assets/images/my.jpg"><link rel="stylesheet" href="/assets/css/style-sxklfps8ywgfyyjcowvnb4gxdgt0zjts3hsguljmv9uqanxjbnitrovtbrek.min.css"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?21513ec2bcd577b3297a1b16da82fa40";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body><div id="blog"><header id="header" data-behavior="4"><i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i><div class="header-title"><a class="header-title-link" href="/">Hi!Roy!</a></div><a class="header-right-icon" href="#about"><i class="fa fa-lg fa-question"></i></a></header><nav id="sidebar" data-behavior="4"><div class="sidebar-container"><div class="sidebar-profile"><a href="/#about"><img class="sidebar-profile-picture" src="/assets/images/my.jpg" alt="作者的图片"></a><h4 class="sidebar-profile-name">Roy</h4><h5 class="sidebar-profile-bio"><p>君以国士待我，我必以国士报君。</p></h5></div><ul class="sidebar-buttons"><li class="sidebar-button"><a class="sidebar-button-link" href="/"><i class="sidebar-button-icon fa fa-lg fa-home"></i> <span class="sidebar-button-desc">首页</span></a></li><li class="sidebar-button"><a class="sidebar-button-link" href="/all-categories"><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i> <span class="sidebar-button-desc">分类</span></a></li><li class="sidebar-button"><a class="sidebar-button-link" href="/all-tags"><i class="sidebar-button-icon fa fa-lg fa-tags"></i> <span class="sidebar-button-desc">标签</span></a></li><li class="sidebar-button"><a class="sidebar-button-link" href="/all-archives"><i class="sidebar-button-icon fa fa-lg fa-archive"></i> <span class="sidebar-button-desc">归档</span></a></li><li class="sidebar-button"><a class="sidebar-button-link open-algolia-search" href="#search"><i class="sidebar-button-icon fa fa-lg fa-search"></i> <span class="sidebar-button-desc">搜索</span></a></li><li class="sidebar-button"><a class="sidebar-button-link" href="#about"><i class="sidebar-button-icon fa fa-lg fa-question"></i> <span class="sidebar-button-desc">关于</span></a></li></ul><ul class="sidebar-buttons"><li class="sidebar-button"><a class="sidebar-button-link" href="https://github.com/shenyushun" target="_blank" rel="noopener"><i class="sidebar-button-icon fa fa-lg fa-github"></i> <span class="sidebar-button-desc">GitHub</span></a></li><li class="sidebar-button"><a class="sidebar-button-link" href="mailto:darkcooking@gmail.com" target="_blank" rel="noopener"><i class="sidebar-button-icon fa fa-lg fa-envelope-o"></i> <span class="sidebar-button-desc">邮箱</span></a></li></ul><ul class="sidebar-buttons"><li class="sidebar-button"><a class="sidebar-button-link" href="/atom.xml"><i class="sidebar-button-icon fa fa-lg fa-rss"></i> <span class="sidebar-button-desc">RSS</span></a></li></ul></div></nav><div id="main" data-behavior="4" class="hasCoverMetaIn"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><div class="post-header main-content-wrap text-left"><h1 class="post-title" itemprop="headline">提升keras准确率和速度的小tips</h1><div class="post-meta"><time itemprop="datePublished" datetime="2019-03-14T17:27:14+08:00">3月 14, 2019 </time><span>发布在 </span><a class="category-link" href="/source/all-categories/Python/">Python</a>, <a class="category-link" href="/source/all-categories/Python/机器学习/">机器学习</a></div></div><div class="post-content markdown" itemprop="articleBody"><div class="main-content-wrap"><p>这里记录一下对于新手(对,说的就是本人)学习kears框架时用来提升准确率的一些tip,但这里都是”术”的层面,而对于”道”,还是要看数学.全文以深度学习界的”hello world”-<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">手写数字识别</a>为例.</p><a id="more"></a><p>首先载入所需要的库:<br></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense,Dropout,Activation</span><br></pre></td></tr></table></figure><p></p><p>然后编写函数加载数据:<br></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">  (x_train,y_train),(x_test,y_test) = mnist.load_data()</span><br><span class="line">  number = <span class="number">10000</span></span><br><span class="line">  x_train = x_train[:number] <span class="comment"># 完整训练数据有6w,这里取前1w</span></span><br><span class="line">  y_train = y_train[:number]  </span><br><span class="line">  x_train = x_train.reshape(number,<span class="number">28</span>*<span class="number">28</span>) <span class="comment"># 原始数据是3维,这里变成2维</span></span><br><span class="line">  x_test=x_test.reshape(x_test.shape[<span class="number">0</span>],<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">  x_train = x_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">  x_test = x_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">  y_train = np_utils.to_categorical(y_train,<span class="number">10</span>) <span class="comment"># 原始数据是1,2...9这样的数字,to_categorical将其变成向量,对应的数字位置为1,其余为0</span></span><br><span class="line">  y_test = np_utils.to_categorical(y_test,<span class="number">10</span>)</span><br><span class="line">  x_train = x_train / <span class="number">255</span></span><br><span class="line">  x_test = x_test / <span class="number">255</span></span><br><span class="line">  <span class="keyword">return</span> (x_train,y_train),(x_test,y_test)</span><br><span class="line"></span><br><span class="line">(x_train,y_train),(x_test,y_test) = load_data()</span><br></pre></td></tr></table></figure><p></p><h2 id="选择合适的loss函数"><a href="#选择合适的loss函数" class="headerlink" title="选择合适的loss函数"></a>选择合适的loss函数</h2><p>对于loss函数,如果之前有学过经典机器学习的小伙伴一定最熟悉MSE(均方误差),所以先用这个实现一个版本:<br></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'mse'</span>,optimizer=SGD(lr=<span class="number">0.1</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">100</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p></p><p>这里activation函数使用<code>sigmoid</code>,optimizer使用SGD(随机梯度下降),loss选择MSE,2个隐藏层,隐藏层节点数量689,程序输出如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.12860000133514404</span><br><span class="line">Test Accc: 0.1362999975681305</span><br></pre></td></tr></table></figure><p></p><p>这里可以看出,基本凉凉.不论是在测试集还是训练集准确度都很低.但是,如果把loss函数换成<code>categorical_crossentropy</code>,输出就变成:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.8550000190734863</span><br><span class="line">Test Accc: 0.8374000191688538</span><br></pre></td></tr></table></figure><p></p><p>很明显的提升,这也说明,MSE对于分类问题不是很有好.</p><h2 id="合适的隐藏层数量"><a href="#合适的隐藏层数量" class="headerlink" title="合适的隐藏层数量"></a>合适的隐藏层数量</h2><p>对于初学者有种幻想,层数越多精度就会越高.这里增加一下层数试试:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'sigmoid'</span>)) <span class="comment"># 来个10层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=SGD(lr=<span class="number">0.1</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">100</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>结果如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.09910000115633011</span><br><span class="line">Test Accc: 0.10320000350475311</span><br></pre></td></tr></table></figure><p></p><p>WTF?!准确度反而降低了??这里其实和<code>sigmoid</code>这个函数有关,这个函数会导致vanish gradient problem.简言之就是使用这个函数进行训练时层数越多,每次参数变化引起结果变化的程度就越小,因为<code>sigmoid</code>函数会把不论大小的输入都转化到0-1这个区间中,具体看其<a href="https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">函数图像</a>就可以明白了.</p><h2 id="合适的激活函数"><a href="#合适的激活函数" class="headerlink" title="合适的激活函数"></a>合适的激活函数</h2><p><code>sigmoid</code>函数其实比较少用了,现在更常用的是<code>relu</code><a href="https://baike.baidu.com/item/ReLU%20%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">函数</a>,可以避免vanish gradient problem.此外,<code>relu</code>其实是<code>Maxout</code>的一个特例:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'relu'</span>)) <span class="comment"># 来个10层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=SGD(lr=<span class="number">0.1</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">100</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>结果如下,可以看到增加10层使用<code>relu</code>函数不受影响.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.9959999918937683</span><br><span class="line">Test Accc: 0.9541000127792358</span><br></pre></td></tr></table></figure><p></p><h2 id="合适的batch-size"><a href="#合适的batch-size" class="headerlink" title="合适的batch_size"></a>合适的batch_size</h2><p>batch_size影响每次训练使用的数据量,比如极端情况下:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'relu'</span>)) <span class="comment"># 来个10层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=SGD(lr=<span class="number">0.1</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">10000</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>这里把batch_size的值改成和整个训练集一样大,结果如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.2732999920845032</span><br><span class="line">Test Accc: 0.26249998807907104</span><br></pre></td></tr></table></figure><p></p><p>又凉凉了,所以 <strong>batch_size</strong> 过大速度快,但会影响精度.而过小则速度会慢,特别是使用GPU的时候,如果这个值设定的过小不能完全发挥GPU的加速功能.</p><h2 id="合适的optimizer"><a href="#合适的optimizer" class="headerlink" title="合适的optimizer"></a>合适的optimizer</h2><p>目前最常用的优化函数是<code>adam</code>,<code>adam=RMSProp+Momentum</code>,这里替换掉SGD:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'relu'</span>)) <span class="comment"># 来个10层</span></span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">100</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出如下,精度差不多但是训练的速度提升了很多:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.9906999754905701</span><br><span class="line">Test Accc: 0.9217000007629395</span><br></pre></td></tr></table></figure><p></p><h2 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h2><p>当训练样本过少时候,往往会出现过拟合的现象,这时可以使用Dropout层来”限制学习能力”.这个方法原理是在每次更新参数之前根据概率丢掉某些neuron,使整个网络结构发生了改变,在每一个mini-batch上重复这个行为,得到不同的结果,相当于训练出了很多个不同的网络,然后再把结果平均得到最终结果(ensemble的理念).这个方法会降低在训练集上的精准度.</p><p>为了模拟过拟合,我们在处理数据集时候添加噪声:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">  (x_train,y_train),(x_test,y_test) = mnist.load_data()</span><br><span class="line">  number = <span class="number">10000</span></span><br><span class="line">  x_train = x_train[:number] <span class="comment"># 完整训练数据有6w,这里取前1w</span></span><br><span class="line">  y_train = y_train[:number]  </span><br><span class="line">  x_train = x_train.reshape(number,<span class="number">28</span>*<span class="number">28</span>) <span class="comment"># 原始数据是3维,这里变成2维</span></span><br><span class="line">  x_test=x_test.reshape(x_test.shape[<span class="number">0</span>],<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">  x_train = x_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">  x_test = x_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">  y_train = np_utils.to_categorical(y_train,<span class="number">10</span>) <span class="comment"># 原始数据是1,2...9这样的数字,to_categorical将其变成向量,对应的数字位置为1,其余为0</span></span><br><span class="line">  y_test = np_utils.to_categorical(y_test,<span class="number">10</span>)</span><br><span class="line">  x_train = x_train / <span class="number">255</span></span><br><span class="line">  x_test = x_test / <span class="number">255</span></span><br><span class="line">  x_test=np.random.normal(x_test) <span class="comment"># 加噪声</span></span><br><span class="line">  <span class="keyword">return</span> (x_train,y_train),(x_test,y_test)</span><br><span class="line"></span><br><span class="line">(x_train,y_train),(x_test,y_test) = load_data()</span><br></pre></td></tr></table></figure><p>然后经过2层<code>relu</code>+<code>adam</code>+<code>categorical_crossentropy</code>+<code>batch_size=100</code>结果如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.9894000291824341</span><br><span class="line">Test Accc:  0.5034999847412109</span><br></pre></td></tr></table></figure><p></p><p>可以看到训练集精度很高而测试集准确度一般,添加Dropout层:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(input_dim=<span class="number">28</span>*<span class="number">28</span>,units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.7</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.7</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">689</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.7</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>,activation=<span class="string">'softmax'</span>)) <span class="comment"># 输出层10个节点</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,optimizer=<span class="string">'adam'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train,y_train,batch_size=<span class="number">100</span>,epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">train_result = model.evaluate(x_train,y_train,batch_size=<span class="number">10000</span>)</span><br><span class="line">test_result = model.evaluate(x_test,y_test,batch_size=<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">'Train Accc:'</span>,train_result[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'Test Accc:'</span>,test_result[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>其中Dropout层添加到每个隐藏层之间,常用的概率值是0.5左右,结果如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.9824030212854031</span><br><span class="line">Test Accc: 0.6224999713897705</span><br></pre></td></tr></table></figure><p></p><p>可以看到测试集精度提升了一些.当然,除了Dropout之外可以<a href="https://keras.io/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore" target="_blank" rel="noopener">Early Stopping</a>和Regularization(正则化,简单说就是使用某种方法使结果越来越接近0,Weight Decay,但这个帮助并不显著.)</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程!"></a>特征工程!</h2><p>在上面的测试准确率可以达到90%以上,但细心的小伙伴应该发现了,在加载数据时候有这样2行代码:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_train / <span class="number">255</span></span><br><span class="line">x_test = x_test / <span class="number">255</span></span><br></pre></td></tr></table></figure><p>这两行代码就是对原始数据进行了缩放,将原始值在0-255之间的数据转化到0-1这个区间.如果没有这个处理,那么依然使用2层<code>relu</code>+<code>adam</code>+<code>categorical_crossentropy</code>+<code>batch_size=100</code>结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Train Accc: 0.10010000318288803</span><br><span class="line">Test Accc: 0.09799999743700027</span><br></pre></td></tr></table></figure><p>准确度居然降到和<code>sigmoid</code>+<code>SGD</code>+<code>mse</code>差不多的程度.所以,如果当程序在训练集上的准确度都很低的话,除了调整参数还需要进一步考虑特征工程是否合理合适了.</p></div></div><div id="post-footer" class="post-footer main-content-wrap"><div class="post-footer-tags"><span class="text-color-light text-small">标签</span><br><a class="tag tag--primary tag--small t-link" href="/source/all-tags/kears/">kears</a> <a class="tag tag--primary tag--small t-link" href="/source/all-tags/python/">python</a></div><div class="post-actions-wrap"><nav><ul class="post-actions post-action-nav"><li class="post-action"><a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/06/Scrapy-Redis结合POST请求获取数据/" data-tooltip="Scrapy-Redis结合POST请求获取数据"><i class="fa fa-angle-left"></i> <span class="hide-xs hide-sm text-small icon-ml">上一篇</span></a></li><li class="post-action"><a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/11/如何对Go代码解偶/" data-tooltip="如何对Go代码解偶"><span class="hide-xs hide-sm text-small icon-mr">下一篇</span> <i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions"><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-google-plus"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-weibo"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/&amp;title=提升keras准确率和速度的小tips"><i class="fa fa-qq"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-star"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-renren"></i></a></li><li class="post-action"><a class="post-action-btn btn btn--default" href="#"><i class="fa fa-list"></i></a></li></ul></div></div></article><footer id="footer" class="main-content-wrap"><span class="copyrights">Copyrights &copy; 2020 Roy. All Rights Reserved.</span></footer></div><div id="bottom-bar" class="post-bottom-bar" data-behavior="4"><div class="post-actions-wrap"><nav><ul class="post-actions post-action-nav"><li class="post-action"><a class="post-action-btn btn btn--default tooltip--top" href="/2019/05/06/Scrapy-Redis结合POST请求获取数据/" data-tooltip="Scrapy-Redis结合POST请求获取数据"><i class="fa fa-angle-left"></i> <span class="hide-xs hide-sm text-small icon-ml">上一篇</span></a></li><li class="post-action"><a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/11/如何对Go代码解偶/" data-tooltip="如何对Go代码解偶"><span class="hide-xs hide-sm text-small icon-mr">下一篇</span> <i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions"><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-google-plus"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-weibo"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/&amp;title=提升keras准确率和速度的小tips"><i class="fa fa-qq"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-star"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-renren"></i></a></li><li class="post-action"><a class="post-action-btn btn btn--default" href="#"><i class="fa fa-list"></i></a></li></ul></div></div><div id="share-options-bar" class="share-options-bar" data-behavior="4"><i id="btn-close-shareoptions" class="fa fa-close"></i><ul class="share-options"><li class="share-option"><a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-facebook-official"></i><span>分享到 Facebook</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-twitter"></i><span>分享到 Twitter</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="https://plus.google.com/share?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-google-plus"></i><span>分享到 Google+</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="http://service.weibo.com/share/share.php?&amp;title=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-weibo"></i><span>分享到 Weibo</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/&amp;title=提升keras准确率和速度的小tips"><i class="fa fa-qq"></i><span>分享到 QQ</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-star"></i><span>分享到 Qzone</span></a></li><li class="share-option"><a class="share-option-btn" target="new" href="http://widget.renren.com/dialog/share?resourceUrl=http://www.hi-roy.com/2019/03/14/提升keras准确率和速度的tips/"><i class="fa fa-renren"></i><span>分享到 Renren</span></a></li></ul></div></div><div id="about"><div id="about-card"><div id="about-btn-close"><i class="fa fa-remove"></i></div><img id="about-card-picture" src="/assets/images/my.jpg" alt="作者的图片"><h4 id="about-card-name">Roy</h4><div id="about-card-bio"><p>君以国士待我，我必以国士报君。</p></div><div id="about-card-job"><i class="fa fa-briefcase"></i><br><p>野生程序猿</p></div><div id="about-card-location"><i class="fa fa-map-marker"></i><br>China</div></div></div><div id="algolia-search-modal" class="modal-container"><div class="modal"><div class="modal-header"><span class="close-button"><i class="fa fa-close"></i></span> <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span> <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg"> </a><i class="search-icon fa fa-search"></i><form id="algolia-search-form"><input type="text" id="algolia-search-input" name="search" class="form-control input--large search-input" placeholder="Search "></form></div><div class="modal-body"><div class="no-result text-color-light text-center">没有找到文章</div><div class="results"><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/02/12/Ubuntu下Gogant的简易破墙术/"><h3 class="media-heading">Ubuntu下Gogant的简易破墙术</h3></a><span class="media-meta"><span class="media-date text-small">2013年2月12日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/10/31/python常用第三方库-转载/"><h3 class="media-heading">Python常用第三方库(转载)</h3></a><span class="media-meta"><span class="media-date text-small">2013年10月31日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/fedora19安装ar8161网卡驱动/"><h3 class="media-heading">fedora19安装ar8161网卡驱动</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/fedora19源，rpmforge，fastestmirror/"><h3 class="media-heading">fedora19源，rpmforge，fastestmirror</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/python中如何自定义解析域名/"><h3 class="media-heading">python中如何自定义解析域名</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/django版本更换/"><h3 class="media-heading">django版本更换</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/django-groundwork个人1-5-3修改版/"><h3 class="media-heading">django-groundwork个人1.5.3修改版</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/fedora19美化/"><h3 class="media-heading">fedora19美化</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/装饰器/"><h3 class="media-heading">装饰器</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div><div class="media"><div class="media-body"><a class="link-unstyled" href="http://www.hi-roy.com/2013/11/01/SVN常用操作/"><h3 class="media-heading">SVN常用操作</h3></a><span class="media-meta"><span class="media-date text-small">2013年11月1日</span></span><div class="media-content hide-xs font-merryweather"></div></div><div style="clear:both"></div><hr></div></div></div><div class="modal-footer"><p class="results-count text-medium" data-message-zero="没有找到文章" data-message-one="找到 1 篇文章" data-message-other="找到 {n} 篇文章">找到 228 篇文章</p></div></div></div><div id="cover" style="background-image:url(/assets/images/cover.jpg)"></div><script src="/assets/js/script-ivwiy10zeb8fifc4swnhkwneuk64y53w2scmdmtp8thi9cqfxh31aowtroaz.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.14.1/moment-with-locales.min.js"></script><script src="//cdn.jsdelivr.net/algoliasearch/3/algoliasearch.min.js"></script><script>var algoliaClient=algoliasearch("51U8PIBLP6","16909d9ce1780cda71113841864e7aa8"),algoliaIndex=algoliaClient.initIndex("my-blog")</script></body></html>